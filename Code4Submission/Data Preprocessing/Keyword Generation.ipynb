{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword Generation\n",
    "This notebook is used to select words from a given set of document that bears close resemblance with a given set of user input words. This is done with the aim of supplementing our current pool of ESG keywords which will ultilmately be used to filter out sentences from our ESG reports\n",
    "\n",
    "Attached is an image illustrating the deconstruction of ESG pillars\n",
    "\n",
    "![esgmsci](https://www.visualcapitalist.com/wp-content/uploads/2021/03/shareable-5.jpg)\n",
    "\n",
    "*Image Credit: www.visualcapitalist.com*\n",
    "\n",
    "For each sub-pillar, we will pick out the keywords and use NLP derive the vector for each word so as to find it's nearest neighbours. This gives us a series of words that are similar/closely related to the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries \n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "import fitz\n",
    "import math\n",
    "import json\n",
    "import pprint\n",
    "import gensim\n",
    "import collections\n",
    "import spacy\n",
    "import nltk\n",
    "import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from gensim.utils import simple_preprocess\n",
    "from textblob import TextBlob\n",
    "from scipy import spatial\n",
    "from gensim.models import Word2Vec\n",
    "from numba import jit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial set of keywords\n",
    "This will be the json file that contains our preliminary set of keywords. They are retrieved manually from articles/ reports. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the key words from our json file\n",
    "f = open('keywords.json')\n",
    "keywordBank = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding and Similarity scoring\n",
    "Given that our initial approach of manually searching for keywords under each component for the 3 pillars will not yield a full representation of the topic at hand, the aim here is to employ Word Embedding NLP technique to filter out words that bears close resemblance to the given word in terms of cosine-similarity distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------- Read a pdf into a large string of text ---------------------------\n",
    "def read_pdf(file_path):\n",
    "    pymupdf_text = \"\"\n",
    "    with fitz.open(file_path) as doc:\n",
    "        for page in doc:\n",
    "            pymupdf_text += page.get_text()\n",
    "    return pymupdf_text\n",
    "\n",
    "\n",
    "# --------------------------- Read a report and breaks it up into individual sentences ---------------------------\n",
    "def convert_pdf_into_sentences(text):\n",
    "    # Remove unnecessary spaces and line breaks\n",
    "    text = re.sub(r'\\x0c\\x0c|\\x0c', \"\", str(text))\n",
    "    text = re.sub('\\n ', '', str(text))\n",
    "    text = re.sub('\\n', ' ', str(text))\n",
    "    text = ' '.join(text.split())\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    if \"”\" in text: text = text.replace(\".”\", \"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\", \"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\", \"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\", \"\\\"?\")\n",
    "    text = text.replace(\".\", \".<stop>\")\n",
    "    text = text.replace(\"?\", \"?<stop>\")\n",
    "    text = text.replace(\"!\", \"!<stop>\")\n",
    "    text = text.replace(\"<prd>\", \".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "\n",
    "    # Filter for sentences with more than 100 characters\n",
    "    sentences = [s.strip() for s in sentences if len(s) > 100]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 468/468 [02:04<00:00,  3.77it/s]\n"
     ]
    }
   ],
   "source": [
    "word_bank = []\n",
    "\n",
    "# Read our database of ESG reports\n",
    "path = 'Reports 2.0'\n",
    "esg_reports = glob.glob(path + '/*.pdf')\n",
    "for report in tqdm.tqdm(esg_reports):\n",
    "    word_bank.append(convert_pdf_into_sentences(read_pdf(report)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:04<00:00, 105.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# Combining all sentence into a set of words\n",
    "cleaned = []\n",
    "for sentence in tqdm.tqdm(word_bank):\n",
    "    combined = ' '.join(sentence)\n",
    "    new_string = re.sub(r\"[^a-zA-Z0-9]\",\" \", combined)\n",
    "    cleaned.append(new_string.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(\n",
    "    sentences=cleaned,\n",
    "    size=100,\n",
    "    alpha=0.025,\n",
    "    window=5,\n",
    "    min_count=5,\n",
    "    workers=4\n",
    ")\n",
    "\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model trained on the corpus of ESG reports\n",
    "model = Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Top 10 words that are similar to: data\n",
      "[('information', 0.7035861015319824),\n",
      " ('Data', 0.6439417600631714),\n",
      " ('endpoint', 0.5328998565673828),\n",
      " ('privacy', 0.5268815755844116),\n",
      " ('cybersecurity', 0.49566537141799927),\n",
      " ('posture', 0.48101022839546204),\n",
      " ('authentication', 0.47874704003334045),\n",
      " ('cyber', 0.475664347410202),\n",
      " ('vulnerabilities', 0.4756225347518921),\n",
      " ('Redundant', 0.4716993272304535)]\n",
      "\n",
      "\n",
      ">>> Top 10 words that are similar to: security\n",
      "[('cybersecurity', 0.7609637379646301),\n",
      " ('privacy', 0.724395215511322),\n",
      " ('cyber', 0.6816527247428894),\n",
      " ('protection', 0.6676135659217834),\n",
      " ('safety', 0.6309021711349487),\n",
      " ('Security', 0.6017462015151978),\n",
      " ('systems', 0.5922068953514099),\n",
      " ('vulnerability', 0.5906700491905212),\n",
      " ('protocols', 0.5839361548423767),\n",
      " ('reliability', 0.5838332176208496)]\n",
      "\n",
      "\n",
      ">>> Top 10 words that are similar to: governance\n",
      "[('responsibility', 0.66522216796875),\n",
      " ('Governance', 0.6651098132133484),\n",
      " ('structure', 0.6410582661628723),\n",
      " ('citizenship', 0.6372106075286865),\n",
      " ('transparency', 0.605177104473114),\n",
      " ('body', 0.5957040786743164),\n",
      " ('policies', 0.5811994075775146),\n",
      " ('oversight', 0.5791769027709961),\n",
      " ('philosophy', 0.5780477523803711),\n",
      " ('accountability', 0.5759695172309875)]\n",
      "\n",
      "\n",
      ">>> Top 10 words that are similar to: carbon\n",
      "[('emission', 0.8036497235298157),\n",
      " ('greenhouse', 0.7939143776893616),\n",
      " ('emissions', 0.7833173871040344),\n",
      " ('neutrality', 0.7729472517967224),\n",
      " ('footprint', 0.7686015367507935),\n",
      " ('gas', 0.7620090842247009),\n",
      " ('CO2', 0.7520161867141724),\n",
      " ('zero', 0.7330183386802673),\n",
      " ('neutral', 0.7326037287712097),\n",
      " ('gases', 0.722003698348999)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test out the model with some basic inputs\n",
    "test_words = ['data', 'security', 'governance', 'carbon']\n",
    "\n",
    "for word in test_words:\n",
    "    sims = model.wv.most_similar(word, topn=10) # Get other similar words\n",
    "    print(f\">>> Top 10 words that are similar to: {word}\")\n",
    "    pprint.pprint(sims)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Parse out all our manually acuqired keywords\n",
    "components_words = []\n",
    "\n",
    "for component, keywords in keywordBank['Environment'].items():\n",
    "    components_words.append(keywords)\n",
    "\n",
    "for component, keywords in keywordBank['Social'].items():\n",
    "    components_words.append(keywords)\n",
    "\n",
    "for component, keywords in keywordBank['Governance'].items():\n",
    "    components_words.append(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword generation\n",
    "\n",
    "To retrieve keywords from the our model closely resembles a given word input, we will be running parsing the entire collection of manual keywords into the model.\n",
    "\n",
    "To avoid the over-retrieval of keywords that may lead to overlaps, we will be limiting the retrieval to the top **3 words.**\n",
    "\n",
    "Based on our *preliminary* testing, we concluded that a threshold of **70%** for similarity sccoring gives us the best keywords that resembles the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 144.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words added: 116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Building on the current list of keywords\n",
    "position = 0\n",
    "count = 0\n",
    "temp = components_words.copy()\n",
    "for wordList in tqdm.tqdm(temp):\n",
    "    newKeywords = []\n",
    "    newKeywords.extend(wordList)\n",
    "    for keyw in wordList:\n",
    "        try:\n",
    "            sims = model.wv.most_similar(keyw, topn=3)\n",
    "            for newWord in sims:\n",
    "                if (newWord[0] not in newKeywords) and (newWord[1] > 0.7):\n",
    "                    count += 1\n",
    "                    newKeywords.append(newWord[0])\n",
    "        except KeyError:\n",
    "            continue\n",
    "    components_words[position] = newKeywords\n",
    "    position += 1\n",
    "print(f\"Number of words added: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Reappend back to our dictionary\n",
    "pointer = 0\n",
    "for pillar, comps in keywordBank.items():\n",
    "    if pillar in ['Environment', 'Social', 'Governance']:\n",
    "        for component, keywords in keywordBank[pillar].items():\n",
    "            keywordBank[pillar][component] = components_words[pointer]\n",
    "            pointer += 1\n",
    "            \n",
    "# Repopulate our json file with the newly added keywords\n",
    "with open(\"keywords.json\", \"w\") as outfile:\n",
    "    json.dump(keywordBank, outfile)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "514f02c4191bf6635aa39c3f7e028852a268bfb4c225c025b4739bc9aac37ae9"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
